{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5aeeaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict, Tuple, List\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3211a169",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    \"\"\"Markov Decision Process class\"\"\"\n",
    "\n",
    "    def __init__(self, states: List, actions: List, transitions: Dict,\n",
    "                 rewards: Dict, gamma: float = 0.9):\n",
    "    \n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.transitions = transitions\n",
    "        self.rewards = rewards\n",
    "        self.gamma = gamma\n",
    "        self.n_states = len(states)\n",
    "        self.n_actions = len(actions)\n",
    "\n",
    "    def get_transition_prob(self, state, action, next_state):\n",
    "        \"\"\"Get transition probability P(s'|s,a)\"\"\"\n",
    "        return self.transitions.get((state, action, next_state), 0.0)\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        \"\"\"Get reward R(s,a,s')\"\"\"\n",
    "        return self.rewards.get((state, action, next_state), 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b88b455",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyEvaluationImprovement:\n",
    "    \"\"\"Policy Evaluation and Improvement algorithms\"\"\"\n",
    "\n",
    "    def __init__(self, mdp: MDP):\n",
    "        self.mdp = mdp\n",
    "\n",
    "    def policy_evaluation(self, policy: Dict, theta: float = 1e-6,\n",
    "                         max_iterations: int = 1000) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Evaluate a policy using iterative policy evaluation\n",
    "\n",
    "        Args:\n",
    "            policy: Dict mapping state -> action\n",
    "            theta: Convergence threshold\n",
    "            max_iterations: Maximum iterations\n",
    "\n",
    "        Returns:\n",
    "            Value function V(s) for all states\n",
    "        \"\"\"\n",
    "        V = np.zeros(self.mdp.n_states)\n",
    "\n",
    "        for iteration in range(max_iterations):\n",
    "            delta = 0\n",
    "            V_new = np.zeros(self.mdp.n_states)\n",
    "\n",
    "            for s_idx, state in enumerate(self.mdp.states):\n",
    "                action = policy[state]\n",
    "                v = 0\n",
    "\n",
    "                # Calculate expected value\n",
    "                for next_state in self.mdp.states:\n",
    "                    s_next_idx = self.mdp.states.index(next_state)\n",
    "                    prob = self.mdp.get_transition_prob(state, action, next_state)\n",
    "                    reward = self.mdp.get_reward(state, action, next_state)\n",
    "                    v += prob * (reward + self.mdp.gamma * V[s_next_idx])\n",
    "\n",
    "                V_new[s_idx] = v\n",
    "                delta = max(delta, abs(V_new[s_idx] - V[s_idx]))\n",
    "\n",
    "            V = V_new.copy()\n",
    "\n",
    "            if delta < theta:\n",
    "                print(f\"Policy evaluation converged in {iteration + 1} iterations\")\n",
    "                break\n",
    "\n",
    "        return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b117bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(self, V: np.ndarray) -> Tuple[Dict, bool]:\n",
    "  \n",
    "    policy = {}\n",
    "    policy_stable = True\n",
    "\n",
    "    for s_idx, state in enumerate(self.mdp.states):\n",
    "        old_action = None\n",
    "        best_action = None\n",
    "        best_value = float('-inf')\n",
    "\n",
    "        # Find best action for this state\n",
    "        for action in self.mdp.actions:\n",
    "            action_value = 0\n",
    "\n",
    "            for next_state in self.mdp.states:\n",
    "                s_next_idx = self.mdp.states.index(next_state)\n",
    "                prob = self.mdp.get_transition_prob(state, action, next_state)\n",
    "                reward = self.mdp.get_reward(state, action, next_state)\n",
    "                action_value += prob * (reward + self.mdp.gamma * V[s_next_idx])\n",
    "\n",
    "            if action_value > best_value:\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "\n",
    "        policy[state] = best_action\n",
    "\n",
    "    return policy, policy_stable\n",
    "\n",
    "# Add this method to PolicyEvaluationImprovement class\n",
    "PolicyEvaluationImprovement.policy_improvement = policy_improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85caf06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(self, initial_policy: Dict = None,\n",
    "                    theta: float = 1e-6) -> Tuple[Dict, np.ndarray]:\n",
    " \n",
    "    # Initialize random policy if not provided\n",
    "    if initial_policy is None:\n",
    "        policy = {state: np.random.choice(self.mdp.actions)\n",
    "                 for state in self.mdp.states}\n",
    "    else:\n",
    "        policy = initial_policy.copy()\n",
    "\n",
    "    iteration = 0\n",
    "    print(\"\\n=== Policy Iteration ===\")\n",
    "\n",
    "    while True:\n",
    "        iteration += 1\n",
    "        print(f\"\\nIteration {iteration}\")\n",
    "\n",
    "        # Policy Evaluation\n",
    "        V = self.policy_evaluation(policy, theta)\n",
    "\n",
    "        # Policy Improvement\n",
    "        new_policy, policy_stable = self.policy_improvement(V)\n",
    "\n",
    "        # Check if policy is stable\n",
    "        is_same = all(policy[s] == new_policy[s] for s in self.mdp.states)\n",
    "\n",
    "        if is_same:\n",
    "            print(f\"\\nPolicy converged in {iteration} iterations!\")\n",
    "            break\n",
    "\n",
    "        policy = new_policy\n",
    "\n",
    "    return policy, V\n",
    "\n",
    "# Add this method to PolicyEvaluationImprovement class\n",
    "PolicyEvaluationImprovement.policy_iteration = policy_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55619201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(self, theta: float = 1e-6,\n",
    "                   max_iterations: int = 1000) -> Tuple[Dict, np.ndarray]:\n",
    "   \n",
    "    V = np.zeros(self.mdp.n_states)\n",
    "\n",
    "    print(\"\\n=== Value Iteration ===\")\n",
    "\n",
    "    for iteration in range(max_iterations):\n",
    "        delta = 0\n",
    "        V_new = np.zeros(self.mdp.n_states)\n",
    "\n",
    "        for s_idx, state in enumerate(self.mdp.states):\n",
    "            # Find maximum value over all actions\n",
    "            action_values = []\n",
    "\n",
    "            for action in self.mdp.actions:\n",
    "                action_value = 0\n",
    "\n",
    "                for next_state in self.mdp.states:\n",
    "                    s_next_idx = self.mdp.states.index(next_state)\n",
    "                    prob = self.mdp.get_transition_prob(state, action, next_state)\n",
    "                    reward = self.mdp.get_reward(state, action, next_state)\n",
    "                    action_value += prob * (reward + self.mdp.gamma * V[s_next_idx])\n",
    "\n",
    "                action_values.append(action_value)\n",
    "\n",
    "            V_new[s_idx] = max(action_values)\n",
    "            delta = max(delta, abs(V_new[s_idx] - V[s_idx]))\n",
    "\n",
    "        V = V_new.copy()\n",
    "\n",
    "        if delta < theta:\n",
    "            print(f\"Value iteration converged in {iteration + 1} iterations\")\n",
    "            break\n",
    "\n",
    "    # Extract optimal policy\n",
    "    policy = {}\n",
    "    for s_idx, state in enumerate(self.mdp.states):\n",
    "        best_action = None\n",
    "        best_value = float('-inf')\n",
    "\n",
    "        for action in self.mdp.actions:\n",
    "            action_value = 0\n",
    "\n",
    "            for next_state in self.mdp.states:\n",
    "                s_next_idx = self.mdp.states.index(next_state)\n",
    "                prob = self.mdp.get_transition_prob(state, action, next_state)\n",
    "                reward = self.mdp.get_reward(state, action, next_state)\n",
    "                action_value += prob * (reward + self.mdp.gamma * V[s_next_idx])\n",
    "\n",
    "            if action_value > best_value:\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "\n",
    "        policy[state] = best_action\n",
    "\n",
    "    return policy, V\n",
    "\n",
    "# Add this method to PolicyEvaluationImprovement class\n",
    "PolicyEvaluationImprovement.value_iteration = value_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01c9600c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gridworld_mdp(grid_size: int = 4) -> MDP:\n",
    "   \n",
    "    states = [(i, j) for i in range(grid_size) for j in range(grid_size)]\n",
    "    actions = ['up', 'down', 'left', 'right']\n",
    "\n",
    "    # Define obstacles and goal\n",
    "    obstacles = [(1, 1), (1, 3)]\n",
    "    goal = (3, 3)\n",
    "\n",
    "    # Remove obstacles from states\n",
    "    states = [s for s in states if s not in obstacles]\n",
    "\n",
    "    transitions = {}\n",
    "    rewards = {}\n",
    "\n",
    "    # Define action effects\n",
    "    action_effects = {\n",
    "        'up': (-1, 0),\n",
    "        'down': (1, 0),\n",
    "        'left': (0, -1),\n",
    "        'right': (0, 1)\n",
    "    }\n",
    "\n",
    "    for state in states:\n",
    "        for action in actions:\n",
    "            effect = action_effects[action]\n",
    "            next_state = (state[0] + effect[0], state[1] + effect[1])\n",
    "\n",
    "            # Check if next state is valid\n",
    "            if (next_state in states and\n",
    "                0 <= next_state[0] < grid_size and\n",
    "                0 <= next_state[1] < grid_size):\n",
    "                transitions[(state, action, next_state)] = 1.0\n",
    "            else:\n",
    "                # Stay in place if invalid move\n",
    "                transitions[(state, action, state)] = 1.0\n",
    "                next_state = state\n",
    "\n",
    "            # Set rewards\n",
    "            if next_state == goal:\n",
    "                rewards[(state, action, next_state)] = 10.0\n",
    "            else:\n",
    "                rewards[(state, action, next_state)] = -0.1\n",
    "\n",
    "    return MDP(states, actions, transitions, rewards, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "806bc54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(mdp: MDP, policy: Dict, V: np.ndarray):\n",
    "    \"\"\"Print policy and value function\"\"\"\n",
    "    print(\"\\n=== Optimal Policy ===\")\n",
    "    for i, state in enumerate(mdp.states):\n",
    "        print(f\"State {state}: {policy[state]} (Value: {V[i]:.3f})\")\n",
    "\n",
    "    print(f\"\\nTotal states: {len(mdp.states)}\")\n",
    "    print(f\"Discount factor: {mdp.gamma}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ee6d5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Gridworld MDP...\n"
     ]
    }
   ],
   "source": [
    "# Create gridworld MDP\n",
    "print(\"Creating Gridworld MDP...\")\n",
    "mdp = create_gridworld_mdp(grid_size=4)\n",
    "\n",
    "# Initialize solver\n",
    "solver = PolicyEvaluationImprovement(mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "770fe220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "METHOD 1: POLICY ITERATION\n",
      "==================================================\n",
      "\n",
      "=== Policy Iteration ===\n",
      "\n",
      "Iteration 1\n",
      "Policy evaluation converged in 154 iterations\n",
      "\n",
      "Iteration 2\n",
      "Policy evaluation converged in 154 iterations\n",
      "\n",
      "Iteration 3\n",
      "Policy evaluation converged in 154 iterations\n",
      "\n",
      "Iteration 4\n",
      "Policy evaluation converged in 154 iterations\n",
      "\n",
      "Iteration 5\n",
      "Policy evaluation converged in 154 iterations\n",
      "\n",
      "Iteration 6\n",
      "Policy evaluation converged in 154 iterations\n",
      "\n",
      "Iteration 7\n",
      "Policy evaluation converged in 154 iterations\n",
      "\n",
      "Policy converged in 7 iterations!\n",
      "\n",
      "=== Optimal Policy ===\n",
      "State (0, 0): down (Value: 58.639)\n",
      "State (0, 1): right (Value: 65.266)\n",
      "State (0, 2): down (Value: 72.629)\n",
      "State (0, 3): left (Value: 65.266)\n",
      "State (1, 0): down (Value: 65.266)\n",
      "State (1, 2): down (Value: 80.810)\n",
      "State (2, 0): down (Value: 72.629)\n",
      "State (2, 1): down (Value: 80.810)\n",
      "State (2, 2): down (Value: 89.900)\n",
      "State (2, 3): down (Value: 100.000)\n",
      "State (3, 0): right (Value: 80.810)\n",
      "State (3, 1): right (Value: 89.900)\n",
      "State (3, 2): right (Value: 100.000)\n",
      "State (3, 3): down (Value: 100.000)\n",
      "\n",
      "Total states: 14\n",
      "Discount factor: 0.9\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Policy Iteration\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"METHOD 1: POLICY ITERATION\")\n",
    "print(\"=\"*50)\n",
    "policy_pi, V_pi = solver.policy_iteration()\n",
    "print_results(mdp, policy_pi, V_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "049e9d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "METHOD 2: VALUE ITERATION\n",
      "==================================================\n",
      "\n",
      "=== Value Iteration ===\n",
      "Value iteration converged in 154 iterations\n",
      "\n",
      "=== Optimal Policy ===\n",
      "State (0, 0): down (Value: 58.639)\n",
      "State (0, 1): right (Value: 65.266)\n",
      "State (0, 2): down (Value: 72.629)\n",
      "State (0, 3): left (Value: 65.266)\n",
      "State (1, 0): down (Value: 65.266)\n",
      "State (1, 2): down (Value: 80.810)\n",
      "State (2, 0): down (Value: 72.629)\n",
      "State (2, 1): down (Value: 80.810)\n",
      "State (2, 2): down (Value: 89.900)\n",
      "State (2, 3): down (Value: 100.000)\n",
      "State (3, 0): right (Value: 80.810)\n",
      "State (3, 1): right (Value: 89.900)\n",
      "State (3, 2): right (Value: 100.000)\n",
      "State (3, 3): down (Value: 100.000)\n",
      "\n",
      "Total states: 14\n",
      "Discount factor: 0.9\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Value Iteration\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"METHOD 2: VALUE ITERATION\")\n",
    "print(\"=\"*50)\n",
    "policy_vi, V_vi = solver.value_iteration()\n",
    "print_results(mdp, policy_vi, V_vi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
