{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23243ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb9b6c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandit:\n",
    "    \"\"\"A single bandit arm with a true reward probability.\"\"\"\n",
    "\n",
    "    def __init__(self, true_mean: float):\n",
    "        self.true_mean = true_mean\n",
    "        self.estimated_mean = 0.0\n",
    "        self.num_pulls = 0\n",
    "\n",
    "    def pull(self) -> float:\n",
    "        \"\"\"Pull the arm and get a reward (1 or 0 with probability true_mean).\"\"\"\n",
    "        return 1 if np.random.random() < self.true_mean else 0\n",
    "\n",
    "    def update(self, reward: float):\n",
    "        \"\"\"Update the estimated mean after receiving a reward.\"\"\"\n",
    "        self.num_pulls += 1\n",
    "        # Incremental update formula\n",
    "        self.estimated_mean += (reward - self.estimated_mean) / self.num_pulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "519ce7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiArmedBandit:\n",
    "    \"\"\"Multi-Armed Bandit environment with multiple arms.\"\"\"\n",
    "\n",
    "    def __init__(self, probabilities: List[float]):\n",
    "        self.bandits = [Bandit(p) for p in probabilities]\n",
    "        self.num_arms = len(probabilities)\n",
    "        self.best_arm = np.argmax(probabilities)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset all bandits.\"\"\"\n",
    "        for bandit in self.bandits:\n",
    "            bandit.estimated_mean = 0.0\n",
    "            bandit.num_pulls = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0688b4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy:\n",
    "    \"\"\"Epsilon-Greedy algorithm for MAB.\"\"\"\n",
    "\n",
    "    def __init__(self, epsilon: float = 0.1):\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def select_arm(self, mab: MultiArmedBandit) -> int:\n",
    "        \"\"\"Select an arm using epsilon-greedy strategy.\"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            # Explore: random arm\n",
    "            return np.random.randint(mab.num_arms)\n",
    "        else:\n",
    "            # Exploit: best known arm\n",
    "            return np.argmax([b.estimated_mean for b in mab.bandits])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2022ea73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCB:\n",
    "    \"\"\"Upper Confidence Bound algorithm for MAB.\"\"\"\n",
    "\n",
    "    def __init__(self, c: float = 2.0):\n",
    "        self.c = c\n",
    "        self.t = 0\n",
    "\n",
    "    def select_arm(self, mab: MultiArmedBandit) -> int:\n",
    "        \"\"\"Select an arm using UCB strategy.\"\"\"\n",
    "        self.t += 1\n",
    "\n",
    "        # Pull each arm once first\n",
    "        for i, bandit in enumerate(mab.bandits):\n",
    "            if bandit.num_pulls == 0:\n",
    "                return i\n",
    "\n",
    "        # Calculate UCB values\n",
    "        ucb_values = []\n",
    "        for bandit in mab.bandits:\n",
    "            exploration_bonus = self.c * np.sqrt(np.log(self.t) / bandit.num_pulls)\n",
    "            ucb_values.append(bandit.estimated_mean + exploration_bonus)\n",
    "\n",
    "        return np.argmax(ucb_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6e98136",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThompsonSampling:\n",
    "    \"\"\"Thompson Sampling algorithm for MAB using Beta distribution.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.alpha = None\n",
    "        self.beta = None\n",
    "\n",
    "    def initialize(self, num_arms: int):\n",
    "        \"\"\"Initialize prior parameters (uniform prior).\"\"\"\n",
    "        self.alpha = np.ones(num_arms)\n",
    "        self.beta = np.ones(num_arms)\n",
    "\n",
    "    def select_arm(self, mab: MultiArmedBandit) -> int:\n",
    "        \"\"\"Select an arm by sampling from Beta distributions.\"\"\"\n",
    "        if self.alpha is None:\n",
    "            self.initialize(mab.num_arms)\n",
    "\n",
    "        # Sample from Beta distribution for each arm\n",
    "        samples = [np.random.beta(self.alpha[i], self.beta[i])\n",
    "                   for i in range(mab.num_arms)]\n",
    "        return np.argmax(samples)\n",
    "\n",
    "    def update(self, arm: int, reward: float):\n",
    "        \"\"\"Update Beta distribution parameters.\"\"\"\n",
    "        self.alpha[arm] += reward\n",
    "        self.beta[arm] += (1 - reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cac2dcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(mab: MultiArmedBandit, algorithm, num_steps: int = 1000) -> Tuple[List[float], List[float], List[int]]:\n",
    "    \"\"\"Run a single experiment with given algorithm.\"\"\"\n",
    "    mab.reset()\n",
    "\n",
    "    rewards = []\n",
    "    cumulative_rewards = []\n",
    "    optimal_actions = []\n",
    "    total_reward = 0\n",
    "    optimal_count = 0\n",
    "\n",
    "    # Special handling for Thompson Sampling\n",
    "    if isinstance(algorithm, ThompsonSampling):\n",
    "        algorithm.alpha = None\n",
    "        algorithm.beta = None\n",
    "\n",
    "    # Special handling for UCB\n",
    "    if isinstance(algorithm, UCB):\n",
    "        algorithm.t = 0\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        # Select arm\n",
    "        arm = algorithm.select_arm(mab)\n",
    "\n",
    "        # Pull arm and get reward\n",
    "        reward = mab.bandits[arm].pull()\n",
    "\n",
    "        # Update bandit estimate\n",
    "        mab.bandits[arm].update(reward)\n",
    "\n",
    "        # Update Thompson Sampling parameters\n",
    "        if isinstance(algorithm, ThompsonSampling):\n",
    "            algorithm.update(arm, reward)\n",
    "\n",
    "        # Track metrics\n",
    "        total_reward += reward\n",
    "        rewards.append(reward)\n",
    "        cumulative_rewards.append(total_reward)\n",
    "\n",
    "        if arm == mab.best_arm:\n",
    "            optimal_count += 1\n",
    "        optimal_actions.append(optimal_count / (step + 1) * 100)\n",
    "\n",
    "    return rewards, cumulative_rewards, optimal_actions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
